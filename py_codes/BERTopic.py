import torch
from bertopic import BERTopic
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sentence_transformers import SentenceTransformer
import pandas as pd
import numpy as np
import random
import re
import nltk
import umap
from tqdm import tqdm
from nltk.corpus import stopwords
import os
import pickle

# ì „ì²˜ë¦¬ ë„êµ¬ ì¤€ë¹„
nltk.download("stopwords")
stop_words = set(stopwords.words("english"))

# ì‚¬ìš©ì ì´ë¦„ ì¶”ì¶œ í•¨ìˆ˜
def extract_user_name(file_path):
    base_name = os.path.basename(file_path)
    name_without_ext = os.path.splitext(base_name)[0]
    return name_without_ext

# STEP 1: CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
input_csv = "C:/Users/lsj39/desktop/cs372_project/csv_datas/merged_data_maximum.csv"
df = pd.read_csv(input_csv)

# STEP 2: í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜
def clean_text(text):
    text = re.sub(r"http\S+", "", text)
    text = re.sub(r"\bhttpurl\b|\burl\b", "", text, flags=re.IGNORECASE)
    text = re.sub(r"@\w+", "", text)
    text = re.sub(r"#(\w+)", r"\1", text)
    text = re.sub(r"\brt\b", "", text, flags=re.IGNORECASE)
    text = re.sub(r"[^a-zA-Z\s]", "", text)
    text = re.sub(r"\s+", " ", text).strip()
    text = text.lower().strip()
    doc = nlp(text)
    tokens = [token.lemma_ for token in doc if token.text not in stop_words and not token.is_space]
    return " ".join(tokens)

# STEP 2.5: ì „ì²˜ë¦¬ ì‹¤í–‰ + ìºì‹œ ì €ì¥
cache_dir = "C:/Users/lsj39/desktop/cs372_project/cache3"
cache_csv_path = os.path.join(cache_dir, "cleaned_cache_max.csv")
force_refresh = True

if os.path.exists(cache_csv_path) and not force_refresh:
    print("ì „ì²˜ë¦¬ëœ CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
    cleaned_df = pd.read_csv(cache_csv_path)
    print(f"ì „ì²˜ë¦¬ í›„ ë‚¨ì€ ë¬¸ì„œ ìˆ˜: {len(cleaned_df)}ê°œ")
else:
    print("ğŸ§¹ ì „ì²˜ë¦¬ ì‹¤í–‰ ì¤‘...")
    cleaned_rows = []

    for idx, row in tqdm(df.iterrows(), total=len(df)):
        text = str(row["text"])
        label = row["label"]
        origin_file = row.get("origin_file", "unknown_file")
        user_name = extract_user_name(origin_file)
        cleaned = clean_text(text)

        if 80 >= len(cleaned.strip()) > 60:
            cleaned_rows.append({
                "original_text": text,
                "cleaned_text": cleaned,
                "label": label,
                "user_name": user_name
            })

    cleaned_df = pd.DataFrame(cleaned_rows)
    os.makedirs(cache_dir, exist_ok=True)
    cleaned_df.to_csv(cache_csv_path, index=False, encoding="utf-8-sig")
    print(f" ì „ì²˜ë¦¬ ê²°ê³¼ CSV ì €ì¥ ì™„ë£Œ: {cache_csv_path}")
    print(f" ì „ì²˜ë¦¬ í›„ ë‚¨ì€ ë¬¸ì„œ ìˆ˜: {len(cleaned_rows)}ê°œ")

# STEP 2.6: ìƒ˜í”Œë§
LIMIT = 720000
if len(cleaned_df) > LIMIT:
    sampled_df = cleaned_df.sample(n=LIMIT, random_state=42).reset_index(drop=True)
else:
    sampled_df = cleaned_df

# ë°ì´í„° ë¶„ë¦¬
original_texts = sampled_df["original_text"].tolist()
cleaned_texts = sampled_df["cleaned_text"].tolist()
retained_labels = sampled_df["label"].tolist()
user_names = sampled_df["user_name"].tolist()

# STEP 3: ì„ë² ë”©
embedding_model = SentenceTransformer("all-mpnet-base-v2", device='cuda')
print("ì„ë² ë”© ì¤‘...")
embeddings = embedding_model.encode(cleaned_texts, show_progress_bar=True, batch_size=64)

# STEP 4: UMAP ì°¨ì› ì¶•ì†Œ
print("UMAP ì°¨ì› ì¶•ì†Œ ì¤‘...")
umap_model = umap.UMAP(n_neighbors=10, n_components=40, min_dist=0.0, metric='cosine')
reduced_embeddings = umap_model.fit_transform(embeddings)

# STEP 5: KMeans í´ëŸ¬ìŠ¤í„°ë§
print("KMeans í´ëŸ¬ìŠ¤í„°ë§ ì¤‘...")
kmeans = KMeans(n_clusters=120, verbose=1, random_state=42, n_init="auto")
clusters = kmeans.fit_predict(reduced_embeddings)

sil_score = silhouette_score(reduced_embeddings, clusters)
print(f"\n[ ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´] {sil_score:.4f}")

# STEP 6: BERTopic ëª¨ë¸ë§
topic_model = BERTopic(
    embedding_model=embedding_model,
    umap_model=umap_model,
    calculate_probabilities=False,
    min_topic_size=80,
    verbose=True
)

print("BERTopic êµ¬ì¡° ì´ˆê¸°í™” ì¤‘...")
topic_model.fit(cleaned_texts, embeddings)
_, topics = topic_model.fit_transform(cleaned_texts, embeddings)

# STEP 7: ê²°ê³¼ ì €ì¥
topic_info = topic_model.get_topic_info()
topic_name_map = {row["Topic"]: row["Name"] for _, row in topic_info.iterrows()}
valid_topics = set(range(kmeans.n_clusters))

final_data = [
    (orig, clean, label, uname, topic, topic_name_map.get(topic, "Unknown"))
    for orig, clean, label, uname, topic in zip(
        original_texts, cleaned_texts, retained_labels, user_names, topic_model.topics_
    )
    if topic in valid_topics
]

output_df = pd.DataFrame(final_data, columns=[
    'original_text', 'cleaned_text', 'label', 'user_name', 'topic', 'topic_name'
])

output_csv = "C:/Users/lsj39/Desktop/cs372_project/csv_datas/final_cleaned_topic_results_maximum_samelength2.csv"
output_df.to_csv(output_csv, index=False, encoding='utf-8-sig')

# STEP 8: ì‹œê°í™” ë° ìš”ì•½
print("\n[ í† í”½ ìš”ì•½]")
print(topic_info)
topic_model.visualize_topics().show()
