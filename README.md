# Implicit detection 
# Topic Modeling with BERTopic
## Overview
This project performs topic modeling on a large corpus of textual data using BERTopic. It includes preprocessing, embedding generation using Sentence-BERT, dimensionality reduction with UMAP, clustering via KMeans, and topic extraction using BERTopic. The final result includes both topic assignments and a visualization of the discovered topics.
## Required Files and Dependencies
### Input Data
- CSV file path:  
  ./csv_datas/merged_data_maximum.csv

- Required columns in the CSV:
  - text: the main text content
  - label: a category or label associated with the text
  - origin_file (optional): used to extract the user name from file path

### Pre-trained Models and Tools
- SentenceTransformer model: all-mpnet-base-v2
- Additional resources:
  - NLTK English stopwords
  - UMAP for dimensionality reduction

## Directory Structure
An example directory layout for successful execution:
cs372_project/  
├── cache/  
│   └── cleaned_cache_max.csv  
├── csv_datas/  
│   ├── merged_data_maximum.csv  
│   └── final_cleaned_topic_results_maximum.csv  
└── topic_modeling_script.py

## How to Run
Run the script with the following command:
python topic_modeling_script.py
The script performs the following steps:

1. Loads and optionally filters the dataset  
2. Cleans and lemmatizes text using NLTK  
3. Applies a minimum/maximum length filter to cleaned texts  
4. Saves preprocessed data to a cache file for future reuse  
5. Samples a subset of the data if it exceeds 1,000,000 entries  
6. Generates embeddings using Sentence-BERT  
7. Reduces dimensions using UMAP  
8. Clusters the data with KMeans (120 clusters)  
9. Applies BERTopic to discover topic representations  
10. Saves the results to a CSV file  
11. Displays an interactive topic map using BERTopic visualization

## Output

### CSV Output

The resulting CSV is saved to the following location:

./csv_datas/final_cleaned_topic_results_maximum.csv

This file includes the following columns:

- original_text: the raw input text  
- cleaned_text: the lemmatized and filtered text  
- label: the original label from the dataset  
- user_name: extracted from the origin_file column  
- topic: the topic ID assigned by BERTopic  
- topic_name: the representative name for each topic

### Visualization

The script opens a browser window displaying the topic clusters as an interactive plot generated by BERTopic.

## Installation Requirements

Install the following dependencies before running the script:

pip install pandas numpy nltk umap-learn tqdm scikit-learn sentence-transformers bertopic  
python -m nltk.downloader stopwords  

## Configuration Options

- To skip the cache and reprocess the text, set the following in the script:  
  force_refresh = True

- To change the number of clusters for KMeans, modify this line:  
  kmeans = KMeans(n_clusters=120, ...)

- To adjust the minimum topic size in BERTopic, modify this parameter:  
  min_topic_size = 80

## Notes

- This script is optimized for large-scale datasets and leverages GPU acceleration for embedding generation when available.  
- Ensure that your system has sufficient memory and GPU resources to run all steps efficiently.

## Input Configuration in Script

The main input variable in the script is the path to the CSV file. This is defined near the top of the script using the following line:

input_csv = "./csv_datas/merged_data_maximum.csv"

If you want to use a different input file, modify this line with the appropriate relative path.

### Cache Configuration

The script uses a caching mechanism to avoid redundant preprocessing. The cache path is defined as:

cache_dir = "./cache"
cache_csv_path = os.path.join(cache_dir, "cleaned_cache_max.csv")
force_refresh = True

To enable reuse of existing preprocessed data, set `force_refresh = False`.  
If `True`, preprocessing will run again even if a cache file exists.

### Sampling Limit

To handle large datasets efficiently, the script limits the number of documents as follows:

LIMIT = 1000000

Modify this value to increase or decrease the number of samples processed.


# Model Finetuning

## Overview

This repository contains code for fine-tuning a pre-trained language model for classification tasks using topic-controlled depression detection data. The model focuses on the role of stopwords and their impact on classification performance.

We experiment with three settings:

* **Stopwords Removed**
* **Stopwords Maintained**
* **Stopwords Emphasized** (transformed into special tokens like `[preposition]`, `[conjunction]`, etc.)

## Required Files and Dependencies

### Input Data

* A CSV dataset (`filtered_dataset.csv`) with at least the following columns:

  * `text`: user-written response
  * `label`: binary label indicating depression (1) or not (0)
  * `topic`: (optional) topic tag used for filtering or control

### Pre-trained Models and Tools

* Transformers (HuggingFace)
* Tokenizer and model: e.g., `distilbert-base-uncased` or `bert-base-uncased`
* Google Colab Pro(Tesla T4)

## How to Run
Upload this jupyter file to colab, mount your google drive, and upload the dataset from topic_modeling.
The filename should be changed into final_dataset.csv

## Output
* Trained model saved in `./outputs/` directory (e.g., `/content/drive/MyDrive/results_selected_250613` currently)
* Plots showing:
  * Accuracy and F1 score across different settings
